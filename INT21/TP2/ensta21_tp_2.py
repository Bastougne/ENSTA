# -*- coding: utf-8 -*-
"""ENSTA21-TP-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s4PbixBNb5Ql7wgqxRqPE6irr4TIAgNW

# TP2 - Inspyred and multi-objective optimization

In this series of exercises, you will first learn the basics of the Python module **inspyred**, and then practice on a few multi-objective problems. The reason we are using inspyred instead of implementing the evolutionary algorithms from scratch is: i. inspyred makes it faster to implement variations of evolutionary algorithms; ii. there a few parts of the algorithms that are relatively difficult to implement (for example, the *crowding distance* of NSGA-II), and there is a risk of spending too much time on a detail, instead of learning more interesting general principles.

## Part I: inspyred
The **inspyred** module is not installed by default in Google Colaboratory, so we will first need to execute the line below to install it. You will only need to run this block of code once.
"""

!pip install inspyred

"""Now, we can start working with inspyred. We will begin with a simple example, that shows the operating structure of the module. First, we create an object of type **EvolutionaryComputation**, that implements the evolutionary loop. This object allows us to specify each part of the evolutionary algorithm: 

1. the **selector** specifies the way to select the parents for reproduction (e.g. using tournament selection, or roulette wheel); 
2. the **variator** specifies a list of genetic operators (e.g. mutation, crossover), that will all be applied, one after the other, on the parents that are selected;
3. the **replacer** indicates the type of replacement/slaughtering of the worst individuals that we would like to apply (e.g. $\mu+\lambda$, or $\mu,\lambda$);
4. the **terminators** are the stop conditions for the algorithm (e.g. reaching a certain number of generations/iterations, a certain number of evaluations, or just reaching a certain fitness value).

Let's see the example: as the last time, we will start by creating a pseudo-random number generator using the **random** module.
"""

import inspyred
import random

random_number_generator = random.Random()
random_number_generator.seed(42) # remember, seeding the generators with a fixed value ensures that you will always obtain the same sequence of numbers at every run 

# instantiate the evolutionary algorithm object
evolutionary_algorithm = inspyred.ec.EvolutionaryComputation(random_number_generator)
# and now, we specify every part of the evolutionary algorithm
evolutionary_algorithm.selector = inspyred.ec.selectors.tournament_selection # by default, tournament selection has tau=2 (two individuals), but it can be modified (see below)
evolutionary_algorithm.variator = [inspyred.ec.variators.uniform_crossover, inspyred.ec.variators.gaussian_mutation] # the genetic operators are put in a list, and executed one after the other
evolutionary_algorithm.replacer = inspyred.ec.replacers.plus_replacement # "plus" -> "mu+lambda"
evolutionary_algorithm.terminator = inspyred.ec.terminators.evaluation_termination # the algorithm terminates when a given number of evaluations (see below) is reached

"""We are still missing two parts: the function that we want to optimize (called **evaluator** in inspyred terms) and a way to generate the initial (random) population (termed **generator**). The functions can be defined by the user, but they will need to have specific arguments, so that they can be used by the evolutionary_algorithm object."""

import numpy as np
def weierstrass(X):
    b = 2.5
    h = 0.5
    i = 20

    f = 0
    for k in range(1,i+1):
        f2 = 0
        for x in X:
            f2 += 1- np.cos(np.power(b,k)*x)

        f += np.power(b,-h*k)*f2
    return f

# this takes a list of candidate solutions (called "candidates"), 
# a dictionary called "args" (should you need extra information), and returns the value of the Weierstrass function for
# each candidate solution, in a list
def evaluator_weierstrass(candidates, args) :

  list_of_fitness_values = []

  # iterate over all the candidates, run the Weierstrass function, append result to list
  for candidate in candidates :
    fitness_value = weierstrass(candidate)
    list_of_fitness_values.append(fitness_value)

  return list_of_fitness_values

# this function generates initial random points for the Weierstrass function; it needs a random number generator (called "random") and a dictionary or arguments in input,
# to be used by the EvolutionaryComputation object; returns one (randomly generated) candidate individual
def generator_weierstrass(random, args) :
  number_of_dimensions = args["number_of_dimensions"] # the number of dimensions of the problem will be specified later, and put it in "args"
  minimum = args["minimum"] # also, the minimum value of each dimension will be specified later in "args"
  maximum = args["maximum"] # same goes for the maximum value

  # the individual will be a series of "number_of_dimensions" random values, generated between "minimum" and "maximum"
  individual = [ random_number_generator.uniform(minimum, maximum) for x in range(0, number_of_dimensions) ]

  return individual

"""Finally, after all these preparations, we can run the evolutionary algorithm! The **evolve** method of the EvolutionaryComputation object runs the evolution until the **terminator** condition is satisified. Some parameters of the **evolve** method depend on the parameters specified above (e.g. since we choose the **tournament_selection** selector, we can specify **tournament_size** in the arguments, if we would like to have a $\tau$ different from 2, the default value)."""

final_population = evolutionary_algorithm.evolve( 
                               generator = generator_weierstrass, # of course, we need to specify the evaluator
                               evaluator = evaluator_weierstrass, # and the corresponding evaluator
                               pop_size = 100, # size of the population
                               num_selected = 200, # size of the offspring (children individuals)
                               maximize = False, # this is a minimization problem, but inspyred can also manage maximization problem
                               max_evaluations = 2000, # maximum number of evaluations before stopping, used by the terminator               
                              
                               # all arguments specified below, THAT ARE NOT part of the "evolve" method, will be automatically placed in "args"
                               number_of_dimensions = 5, # number of dimensions of the problem, used by "generator_weierstrass"
                               minimum = -1, # minimum value of each dimension, used by "generator_weierstrass"
                               maximum = 1, # maximum value of each dimension, used by "generator_weierstrass"
                              )

# after the evolution is over, the resulting population is stored in "final_population"; the best individual is on the top
best_individual = final_population[0]
print("The best individual has fitness %.2f" % best_individual.fitness)

"""It would also be nice to see what exactly the evolutionary algorithm is doing at each iteration. inspyred allows us to define a function with specific parameters, called **observer**, that is called at the end of each generation, and can, for example, print out statistics, or save the current population to a file.

Let's see the previous example, this time with an observer that prints out the best fitness, and a few more exposed parameters that can be modified. Also, after the first run, try commenting the line with **best_observer** and uncommenting one of the others, to try the different types of observers that inspyred can offer.
"""

evolutionary_algorithm.observer = inspyred.ec.observers.best_observer # prints best individual to screen
# evolutionary_algorithm.observer = inspyred.ec.observers.stats_observer # print out population statistics
# evolutionary_algorithm.observer = inspyred.ec.observers.plot_observer # plots evolution
final_population = evolutionary_algorithm.evolve( 
                               generator = generator_weierstrass, # of course, we need to specify the evaluator
                               evaluator = evaluator_weierstrass, # and the corresponding evaluator
                               pop_size = 100, # size of the population
                               num_selected = 200, # size of the offspring (children individuals)
                               maximize = False, # this is a minimization problem, but inspyred can also manage maximization problem
                               max_evaluations = 2000, # maximum number of evaluations before stopping, used by the terminator
                               tournament_size = 2, # size of the tournament selection; we need to specify it only if we need it different from 2
                               crossover_rate = 1.0, # probability of applying crossover
                               mutation_rate = 0.1, # probability of applying mutation
                               
                               # all arguments specified below, THAT ARE NOT part of the "evolve" method, will be automatically placed in "args"
                               number_of_dimensions = 5, # number of dimensions of the problem, used by "generator_weierstrass"
                               minimum = -1, # minimum value of each dimension, used by "generator_weierstrass"
                               maximum = 1, # maximum value of each dimension, used by "generator_weierstrass"
                              )

# after the evolution is over, the resulting population is stored in "final_population"; the best individual is on the top
best_individual = final_population[0]
print("The best individual has fitness %.2f" % best_individual.fitness)

"""One of the nicest features of inspyred is the possibility of writing personalized functions, that can replace the ones that inspyred offers. For example, we can write our own observer: the only requirement is that the observer function we write must take specific arguments, with specific names."""

def my_observer(population, num_generations, num_evaluations, args) :
  print("After %d generations and %d evaluations, the fitness of the best individual is %.2f" % (num_generations, num_evaluations, population[0].fitness))
  return

evolutionary_algorithm.observer = my_observer
final_population = evolutionary_algorithm.evolve( 
                               generator = generator_weierstrass, # of course, we need to specify the evaluator
                               evaluator = evaluator_weierstrass, # and the corresponding evaluator
                               pop_size = 100, # size of the population
                               num_selected = 200, # size of the offspring (children individuals)
                               maximize = False, # this is a minimization problem, but inspyred can also manage maximization problem
                               max_evaluations = 2000, # maximum number of evaluations before stopping, used by the terminator
                               tournament_size = 2, # size of the tournament selection; we need to specify it only if we need it different from 2
                               crossover_rate = 1.0, # probability of applying crossover
                               mutation_rate = 0.1, # probability of applying mutation
                               
                               # all arguments specified below, THAT ARE NOT part of the "evolve" method, will be automatically placed in "args"
                               number_of_dimensions = 5, # number of dimensions of the problem, used by "generator_weierstrass"
                               minimum = -1, # minimum value of each dimension, used by "generator_weierstrass"
                               maximum = 1, # maximum value of each dimension, used by "generator_weierstrass"
                              )

# after the evolution is over, the resulting population is stored in "final_population"; the best individual is on the top
best_individual = final_population[0]
print("The best individual has fitness %.2f" % best_individual.fitness)

"""And just like it is possible to write a custom observer, generator, and evaluator, we can write custom selectors, variators, replacers, or terminators. While it is outside of the scope of this class, if you are interested you can find more information in the [inspyred documentation](https://pythonhosted.org/inspyred/).

In particular:
1. [Documentation on observers](https://pythonhosted.org/inspyred/reference.html#module-observers)
2. [Documentation on replacers](https://pythonhosted.org/inspyred/reference.html#module-replacers)
3. [Documentation on selectors](https://pythonhosted.org/inspyred/reference.html#module-selectors)
4. [Documentation on terminators](https://pythonhosted.org/inspyred/reference.html#module-terminators)
5. [Documentation on variators](https://pythonhosted.org/inspyred/reference.html#variators-solution-variation-methods)

You can also see some [examples provided by the developer of inspyred](https://pythonhosted.org/inspyred/examples.html#custom-evolutionary-computation), showing ready-to-use algorithms.

## Part II: Multi-objective evolutionary algorithms

In this section, we will use inspyred to optimize multi-objective benchmark functions. Let's take a look at our first benchmark function, called ZDT-1.
"""

import math
def zdt1(X) :
	
  f1 = X[0]
  g = 1.0 + (9 / (len(X)-1)) * sum(X[1:])
  f2 = g * ( 1.0 - math.sqrt(f1/g))

  return f1, f2

# the main advantage of this benchmark function is that we already know where the TRUE Pareto front lies; in many real-world cases, we do not know its exact position
# let's take a look at the Pareto front
import matplotlib.pyplot as plt # import package that is used for the plots
x_true_front = np.arange(0, 1, 0.001)
y_true_front = [ (1.0 - math.sqrt(xx)) for xx in x_true_front ]
plt.plot(x_true_front, y_true_front, color='r', label='True Pareto front') # plot red line on the true Pareto front
plt.xlabel("f1")
plt.ylabel("f2")
plt.legend(loc="best")

"""First, let's try to optimize in a naive way, by using single-objective optimization and putting weights on the objectives. We will use an EvolutionaryComputation object, like before, implementing both the generator and the evaluator."""

import sys
def evaluator_zdt1_single_objective(candidates, args) :
  # let's get the weights, they will be stored in the "args" dictionary
  weight_f1 = args["weight_f1"]
  weight_f2 = args["weight_f2"]

  list_of_fitness_values = []
  for candidate in candidates :
    f1, f2 = zdt1(candidate)
    fitness_value = weight_f1 * f1 + weight_f2 * f2 # the two objectives of ZDT1 are aggregated in one single value
    list_of_fitness_values.append(fitness_value)

  return list_of_fitness_values

def generator_zdt1(random, args) :
  number_of_dimensions = args["number_of_dimensions"]
  minimum = args["minimum"]
  maximum = args["maximum"]

  # an individual will be a list of random values in the (minimum, maximum) interval, of length equal to the number_of_dimensions
  individual = [ random.uniform(minimum, maximum) for i in range(0, number_of_dimensions) ]
  return individual

# setting up random number generator
random_number_generator = random.Random()
random_number_generator.seed(42)

# setting up and running evolutionary algorithm
ea_single_objective = inspyred.ec.EvolutionaryComputation(random_number_generator)
ea_single_objective.selector = inspyred.ec.selectors.tournament_selection # by default, tournament selection has tau=2 (two individuals), but it can be modified (see below)
ea_single_objective.variator = [inspyred.ec.variators.uniform_crossover, inspyred.ec.variators.gaussian_mutation] # the genetic operators are put in a list, and executed one after the other
ea_single_objective.replacer = inspyred.ec.replacers.plus_replacement # "plus" -> "mu+lambda"
ea_single_objective.terminator = inspyred.ec.terminators.evaluation_termination # the algorithm terminates when a given number of evaluations (see below) is reached
ea_single_objective.observer = my_observer # print out population statistics

# run evolutionary algorithm
final_population = ea_single_objective.evolve(
                                              generator = generator_zdt1,
                                              evaluator = evaluator_zdt1_single_objective,
                                              pop_size = 100,
                                              num_selected = 200,
                                              max_evaluations = 1000,
                                              maximize = False, # it's a minimization problem
                                              bounder = inspyred.ec.Bounder(0,1), # we add a constraint: solutions cannot have values outside of the (0,1) interval, 
                                                                                  # for any dimension, even when modified by the genetic operators/variators
                                              
                                              # all arguments specified below, THAT ARE NOT part of the "evolve" method, will be automatically placed in "args"
                                              minimum = 0.0,
                                              maximum = 1.0,
                                              number_of_dimensions = 5,
                                              weight_f1 = 0.5, # weight in (0,1) used by the single-objective evaluation to add up the two objectives
                                              weight_f2 = 0.5, # weight in (0,1) used by the single-objective evaluation to add up the two objectives
)

# let's plot the best individual, to see where it is placed with respect to the known Pareto front
f1_best, f2_best = zdt1(final_population[0].candidate) # the "candidate" attribute contains the genome of the individual
plt.plot(f1_best, f2_best, 'bx', label="Best individual")
plt.plot(x_true_front, y_true_front, color='r', label='True Pareto front') # plot red line on the true Pareto front
plt.xlabel("f1")
plt.ylabel("f2")
plt.legend(loc="best")

"""Now, modify the code above to place many different points (for example, 100) near different parts of the Pareto fronts.

As you will see, this will require a considerable computational effort. In the next block of code, we are going to use the inspyred implementation of NSGA-II, that has several parts of the evolutionary algorithm (replacer, selector, variators, ...) already set.
"""

# we can use the same 'generator' we exploited above, but we need to redefine the evaluator
def evaluator_zdt1_multi_objective(candidates, args) :

  list_of_fitness_values = []
  for candidate in candidates :
    f1, f2 = zdt1(candidate)
    list_of_fitness_values.append(inspyred.ec.emo.Pareto( [f1, f2] )) # in this case, for multi-objective optimization we need to create a Pareto fitness object with a list of values

  return list_of_fitness_values

# instance of NSGA2
nsga2 = inspyred.ec.emo.NSGA2(random_number_generator)
nsga2.terminator = inspyred.ec.terminators.evaluation_termination
nsga2.variator = [inspyred.ec.variators.blend_crossover, inspyred.ec.variators.gaussian_mutation]

final_pareto_front = nsga2.evolve(
                                    generator = generator_zdt1,
                                    evaluator = evaluator_zdt1_multi_objective,
                                    pop_size = 100,
                                    num_selected = 200,
                                    max_evaluations = 500,
                                    maximize = False, # it's a minimization problem
                                    bounder = inspyred.ec.Bounder(0,1), # we add a constraint: solutions cannot have values outside of the (0,1) interval, 
                                                                        # for any dimension, even when modified by the genetic operators/variators
                                              
                                    # all arguments specified below, THAT ARE NOT part of the "evolve" method, will be automatically placed in "args"
                                    minimum = 0.0,
                                    maximum = 1.0,
                                    number_of_dimensions = 5,
)

# NSGA-II returned a set of individuals representing the best Pareto front it found. Let's compare it with the true Pareto front.
# for each individual, this time the 'fitness' attribute is a list of values 
x_nsga2 = [ individual.fitness[0] for individual in final_pareto_front ]
y_nsga2 = [ individual.fitness[1] for individual in final_pareto_front ]
plt.plot(x_nsga2, y_nsga2, 'bx', label="NSGA-II individuals")
plt.plot(x_true_front, y_true_front, 'r-', label="True Pareto front")
plt.xlabel("f1")
plt.ylabel("f2")
plt.legend(loc="best")

"""As you can see, some of the points generated by NSGA-II are close to the true Pareto front, but most are not. Modify the parameters of NSGA-II as you see fit, in order to place as many points as possible on the Pareto front.

As a last exercise, try to optimize another benchmark function, ZDT3, that presents a discontinuous Pareto front (but not a discontinuous search space).
"""

def zdt3(X) :
  f1 = X[0]
  g = 1.0 + (9 / (len(X)-1)) * sum(X[1:])
  f2 = g * (1.0 - math.sqrt(f1/g) - (f1/g) * math.sin(10 * math.pi * f1))

  return f1, f2

# the true Pareto front of ZDT3 is quite complex to describe, so the code to create the points is slightly more convoluted than before
x_temp = np.arange(0, 1, 0.001)
y_temp = [ (1.0 - math.sqrt(x) - x * math.sin(10 * math.pi * x)) for x in x_temp ]

x_true_front = []
y_true_front = []

lowest_value = 1.01
for i in range(0, len(x_temp)) :
  if y_temp[i] < lowest_value :
    x_true_front.append(x_temp[i])
    y_true_front.append(y_temp[i])
    lowest_value = y_temp[i]

plt.plot(x_true_front, y_true_front, 'r.', label="True Pareto front, ZDT3")
plt.xlabel("f1")
plt.ylabel("f2")
plt.legend(loc="best")