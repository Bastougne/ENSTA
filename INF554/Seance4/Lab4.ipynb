{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from time import perf_counter\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF554 Lab 4: Spectral Clustering and Feature Selection\n",
    "\n",
    "\n",
    "In this lab we will add the spectral clustering algorithm to our collection on unsupervised learning techniques, which we started to build in Lab 3. We will furthermore explore the $\\chi^2$ measure for feature selection in a supervised classification task. \n",
    "\n",
    "## Spectral Clustering\n",
    "\n",
    "We begin by briefly reviewing the spectral clustering algorithm. For more detail on this algorithm please revisit the slides from lecture 3.\n",
    "\n",
    "In the spectral clustering algorithm we utilise the spectral decomposition of a graph representation matrix such as the random walk Laplacian $L_{rw} = I - D^{-1}A$ to cluster a graph or network. Specifically, the spectral clustering algorithm consists of two steps. First we compute the spectral decomposition of the used graph representation matrix,\n",
    "$$\n",
    "L_{rw} = U \\Lambda U^T.\n",
    "$$\n",
    "Then, in the second step we run the $k$-means algorithm on the rows of the eigenvector matrix $U_k,$ containing the eigenvectors corresponding to the $k$ smallest eigenvalues of $L_{rw},$ to cluster the nodes in our graph.\n",
    "\n",
    "If instead of a graph we only have a set of datapoints then we are still able to use the spectral clustering algorithm by utilising one of several heuristics to infer a graph from the dataset. One such method produces a weighted *fully connected graph by using the Gaussian kernel* to represent edge weights in our inferred graph. Another is to infer a $k$*-nearest neighbours graph*, where two nodes are connected if either of them are in within the $k$ nearest neighbours of the other in Euclidean distance.\n",
    "\n",
    "\n",
    "\n",
    "### Dataset: Three Concentric Circles\n",
    "\n",
    "We begin by generating a variation of the famous synthetic, concentric circles dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "\n",
    "def get_circle(r=1.0, N=150):  \n",
    "    # Use polar coords to get unif dist points  \n",
    "    step = np.pi * 2.0 / N  \n",
    "    t = np.arange(0, np.pi * 2.0, step)  \n",
    "    x_1 = r * np.cos(t)  \n",
    "    x_2 = r * np.sin(t)  \n",
    "    return np.column_stack((x_1, x_2))\n",
    "      \n",
    "def get_noise(stddev=0.2, N=150):  \n",
    "    # 2d gaussian random noise  \n",
    "    x_1 = np.random.normal(0, stddev, N)  \n",
    "    x_2 = np.random.normal(0, stddev, N)  \n",
    "    return np.column_stack((x_1, x_2))    \n",
    "  \n",
    "def generateData(sigma):\n",
    "    inner_circle = get_circle(r=1, N=50) + get_noise(sigma,N=50)\n",
    "    middle_circle = get_circle(r=4, N=400) + get_noise(sigma,N=400)\n",
    "    outer_circle = get_circle(r=6, N=800) + get_noise(sigma, N=800)\n",
    "    return np.vstack([inner_circle, middle_circle, outer_circle])\n",
    " \n",
    " \n",
    "X_low_var = generateData(0.2)\n",
    "\n",
    "X_high_var = generateData(0.3)\n",
    "\n",
    "\n",
    "\n",
    "# Plot data\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "ax[0].scatter(X_low_var[:,0], X_low_var[:,1])\n",
    "ax[0].title.set_text('Dataset low variance')\n",
    "ax[1].scatter(X_high_var[:,0], X_high_var[:,1])\n",
    "ax[1].title.set_text('Dataset high variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Task 1:** Use the [scikit-learn kmeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) implementation to cluster the two datasets using the kmeans algorithm (Note that we already loaded the KMeans function for you in code cell 1). Please store the produced labels in the two variables ```labels_low_var``` and ```labels_high_var```. You may assume knowledge of the true number of clusters, i.e., k=3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_low_var = np.zeros(X_low_var.shape[0])\n",
    "labels_high_var = np.zeros(X_high_var.shape[0])\n",
    "\n",
    "k = 3\n",
    "\n",
    "#Please insert your code for Task 1 here\n",
    "\n",
    "\n",
    "\n",
    "# Plot data\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "ax[0].scatter(X_low_var[:,0], X_low_var[:,1], c=labels_low_var, facecolors=\"none\") \n",
    "ax[0].title.set_text('$k$-Means Clustering')\n",
    "\n",
    "ax[1].scatter(X_high_var[:,0], X_high_var[:,1], c=labels_high_var, facecolors=\"none\") \n",
    "ax[1].title.set_text('$k$-Means Clustering')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Task 2:** Implement the function ```knn_graph``` to construct a k-nearest neighbour graph from a set of data points. Furthermore, fill out the ```gaussianKernel``` function to infer a weighted fully connected graph from the two datasets using the Gaussian kernel as a similarity function. (The Gaussian kernel will be discussed in greater detail in Lab 5, for now it suffices to know that it contains a free parameter $\\sigma$ and is defined as follows $  K_{Gauss}(x_i, x_j) = \\exp\\left( - \\frac{\\| x_i - x_j\\|^2}{2 \\sigma^2}\\right).$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def knn_graph(X, neighbours):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X (np.array, n x p):  data points\n",
    "        neighbours (int): number of nearest neighbours to be considered for each node\n",
    "    \n",
    "    Returns:\n",
    "        A (np.array, n x n): the adjacency matrix of the knn graph\n",
    "    \"\"\"    \n",
    "    \n",
    "    n = X.shape[0]\n",
    "    A = np.zeros((n,n))\n",
    "    \n",
    "    #Please insert your code for Task 2 here        \n",
    "    \n",
    "        \n",
    "    return A\n",
    "\n",
    "\n",
    "def gaussianKernel(X1, X2, sigma = 0.1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X1 (np.array, n_1 x p): a set of n_1 datapoints to be used as the first of the two kernel inputs\n",
    "        X2 (np.array, n_2 x p): a set of n_2 datapoints to be used as the second of the two kernel inputs\n",
    "    \n",
    "    Returns:\n",
    "        K (np.array, n_1 x n_2): a kernel matrix containing the kernel values measuring of all possible data point pairs\n",
    "    \"\"\"    \n",
    "    K = np.zeros((X1.shape[0],X2.shape[0]))\n",
    "     \n",
    "    #Please insert your code for Task 2 here\n",
    "\n",
    "    \n",
    "    return K\n",
    "\n",
    "\n",
    "A_low_var_Gaus = gaussianKernel(X_low_var, X_low_var)\n",
    "A_high_var_Gaus = gaussianKernel(X_high_var, X_high_var)\n",
    "\n",
    "\n",
    "neighbours = 8\n",
    "A_low_var_knn = knn_graph(X_low_var, neighbours)\n",
    "A_high_var_knn =  knn_graph(X_high_var, neighbours)\n",
    "\n",
    "\n",
    "    \n",
    "plt.figure(1, figsize=(15,15))\n",
    "G_high_var = nx.from_numpy_matrix(A_high_var_knn-np.eye(A_high_var_knn.shape[0]))\n",
    "nx.draw_networkx(G_high_var, pos=X_high_var, node_size=10)\n",
    "plt.title(\"%d-nearest neighbour graph for the high variance concentric circles (with self-loops removed)\" %(neighbours))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">**Task 3:** Implement the spectral clustering algorithm and use it to cluster the four graphs obtained in Task 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_clustering(A,k):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        A (np.array, n x n): the adjacency matrix of our graph\n",
    "        k (int): the number of clusters\n",
    "    \n",
    "    Returns:\n",
    "        classes (np.array, n): the inferred class labels of the n nodes\n",
    "        U_k (np.array, nxk): the k eigenvectors used in the \n",
    "    \"\"\"    \n",
    "    #Please insert your code for Task 3 here\n",
    "\n",
    "    \n",
    "    return classes, U_k\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classes_low_var_Gaus, U_k_low_var_Gaus = spectral_clustering(A_low_var_Gaus,3)\n",
    "classes_high_var_Gaus, U_k_high_var_Gaus = spectral_clustering(A_high_var_Gaus,3)\n",
    "\n",
    "classes_low_var_knn, U_k_low_var_knn = spectral_clustering(A_low_var_knn,3)\n",
    "classes_high_var_knn, U_k_high_var_knn = spectral_clustering(A_high_var_knn,3)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(15,15))\n",
    "ax[0,0].scatter(X_low_var[:,0], X_low_var[:,1], c=classes_low_var_Gaus, facecolors=\"none\") \n",
    "ax[0,0].title.set_text('Spectral Clustering on the Gaussian similarity graph')\n",
    "\n",
    "ax[0,1].scatter(X_high_var[:,0], X_high_var[:,1], c=classes_high_var_Gaus, facecolors=\"none\") \n",
    "ax[0,1].title.set_text('Spectral Clustering on the Gaussian similarity graph')\n",
    "\n",
    "ax[1,0].scatter(X_low_var[:,0], X_low_var[:,1], c=classes_low_var_knn, facecolors=\"none\") \n",
    "ax[1,0].title.set_text('Spectral Clustering on the knn graph')\n",
    "\n",
    "ax[1,1].scatter(X_high_var[:,0], X_high_var[:,1], c=classes_high_var_knn, facecolors=\"none\") \n",
    "ax[1,1].title.set_text('Spectral Clustering on the knn graph')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 2) Feature selection using the $\\chi^2$ measure\n",
    "\n",
    "\n",
    "We will now implement and apply the $\\chi^2$ measure to examine its performance in a supervised  classification task. We begin by describing and loading the considered data set.\n",
    "\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The dataset  describes a set of $102$ molecules of which $39$ are judged by human experts to be musks (class 1) and the remaining 63 molecules are judged to be non-musks (class 2). The goal is to learn to predict whether new molecules will be musks or non-musks, a binary classification task. However, the $166$ features that describe these molecules depend upon the exact shape, or conformation, of the molecule. Because bonds can rotate, a single molecule can adopt many different shapes. To generate this data set, all the low-energy conformations of the molecules were generated to produce $6598$ conformations. Then, a feature vector was extracted that describes each conformation. The final dataset has $6598$ instances and $166$ features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "data = np.loadtxt('data/musk.csv', delimiter=',')\n",
    "Y = data[:,0:1].reshape(-1)\n",
    "X = data[:,1:data.shape[1]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=5)\n",
    "\n",
    "print('This is the first datapoint: \\n', X_train[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a logistic regression classifier (which we saw in Lab 2) to classify our molecules. Recall that the predictions produced by the logistic regression classifier are not binary, but in the range $[0,1]$. Hence, we are able to choose a threshold above which we assign the label musk and below which we classify the molecule as non-musk. In order to assess our classifier independent of our chosen threshold we use $101$ different threshold values (from $0$ to $1$ with steps of size $0.01$) to assign class labels. This allows us to plot a *Receiver Operating Characteristic (ROC)* curve to visually compare the quality of our different classifiers. Recall from Lecture 4 that, the ROC curve is produced by plotting the true positive rate (TPR) on the $y$-axis against the false positive rate (FPR) on the $x$-axis as the  threshold varies, where\n",
    "\\begin{align*}\n",
    "\\text{TPR} &= \\frac{\\text{true positive}}{\\text{true positive}+{\\text{false negative}}} \\\\\n",
    "\\text{FPR} &= \\frac{\\text{false positve}}{\\text{false positive}+{\\text{true negative}}}.\n",
    "\\end{align*}\n",
    "\n",
    "ROC curves are evaluated by observing the *Area Under the ROC Curve (AUC).* Models with a large AUC are generally preferable over those with a low AUC.\n",
    "\n",
    "> **Task 4:** Complete the below ```roc_auc``` function by making predictions from the logistic regression model based on the threshold set in the current iteration, calculating the corresponding true (```tpr```), false positive rates (```fpr```) and use these to approximate the area under the ROC Curve (```auc```). This will allow you to use the provided code plotting the ROC curve of the logistic regression classifier applied to the musk dataset with all features present.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc(model, X, Y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model: a trained model\n",
    "        X (np.array, n x p): data\n",
    "        Y (np.array, n): target\n",
    "    \n",
    "    Returns:\n",
    "        tpr (np.array, 101): vector containing the true positive rates corresponding \n",
    "                             to the different classification thresholds\n",
    "        fpr (np.array, 101): vector containing the false positive rates corresponding \n",
    "                             to the different classification thresholds\n",
    "        auc (float): Area Under the ROC Curve\n",
    "    \"\"\"\n",
    "    probs = model.predict_proba(X)\n",
    "    \n",
    "    \n",
    "    num_thresholds = 101\n",
    "    tpr = np.zeros(num_thresholds)\n",
    "    fpr = np.zeros(num_thresholds)\n",
    "    for i in np.arange(101):\n",
    "        threshold = i/100\n",
    "\n",
    "        # Please insert the code for Task 4 here \n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    return tpr, fpr, auc\n",
    "\n",
    "\n",
    "model = LogisticRegression(max_iter=10000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "res = model.predict(X_test) #Note this uses the default threshold set in scikit-learn equal to 0.5\n",
    "tpr, fpr, auc = roc_auc(model, X_test, y_test.astype(\"int\"))\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(fpr, tpr)\n",
    "plt.title(\"Acc = %.2f, AUC = %.2f\" % (np.sum(res==y_test)/len(res), auc))\n",
    "plt.xlabel(\"FPR \")\n",
    "plt.ylabel(\"TPR \")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to consider the $\\chi^2$ measure for feature selection. We work with a data set of $n$ data points $\\{X_i\\}_{i=1}^n,$ where each data point is $p$-dimensional, i.e., we have $p$ features, denoted $X_{\\cdot j}$ for $j \\in \\{1, \\ldots, p\\},$ in our dataset. We furthermore assume that we are working with discrete data where $x_{ij}\\in \\{1, \\ldots, V\\}$ for all $i \\in \\{1, \\dots, n\\}$ and $j\\in \\{1, \\ldots, p\\}.$ Further denote our discrete dependent variable by $Y$ containing class labels of our data points, i.e., $y_i\\in \\{1, \\ldots, C\\}.$ Assume that the features in our dataset $X_{\\cdot j}$ are sampled from the distribution of a random variable $\\mathcal{X}_j$ and the labels in our dataset $Y$ are sampled from the distribution of a random variable $\\mathcal{Y}.$ Then, the $\\chi^2$ measure is defined in terms of the following two quantities\n",
    "\\begin{equation*}\n",
    "\tO_{vc} = \\sum_{i=1}^n \\Big(I(x_{ij} = v) \\cdot I(y_i = c)\\Big), \n",
    "\\end{equation*}\n",
    "which is used to estimate $n \\cdot P(\\mathcal{X}_{j}=v,\\mathcal{Y}=c),$ i.e., the joint probability of feature $\\mathcal{X}_j$ taking value $v$ and $\\mathcal{Y}$ taking value c, and \n",
    " \\begin{equation*}\n",
    "\tE_{vc} =  \\frac{1}{n}\\sum_{i=1}^n I(x_{ij} = v) \\cdot \\sum_{i=1}^n I(y_i = c), \n",
    "\\end{equation*}\n",
    "which estimates the probability $n \\cdot P(\\mathcal{X}_j =v) \\cdot P(\\mathcal{Y}=c),$ i.e., the joint probability of feature $\\mathcal{X}_j$ taking value $v$ and $\\mathcal{Y}$ taking value c if $\\mathcal{X}$ and $\\mathcal{Y}$ are distributed independently.  Now the $\\chi^2$ measure can be defined as  \n",
    "\\begin{equation*}\n",
    "\\chi^2(X_{\\cdot j},Y) = \\sum_{v \\in \\{1, \\ldots V\\}} \\sum_{c \\in \\{1, \\ldots C\\}} \\frac{(O_{vc} - E_{vc})^2}{E_{vc}}.\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "In the case of independence of our data feature $\\mathcal{X}_j$ and the class labels $\\mathcal{Y}$, $O_{vc}$ and $E_{vc}$ should take similar values and we obtain a small $\\chi^2$ measure.\n",
    "If a feature $X_{\\cdot j }$ is close to independent from the labels $Y$ which we want to predict from $X_{\\cdot j },$ then the feature $X_{\\cdot j }$ is not of great help in our learning task. \n",
    "Therefore, small values of the $\\chi^2$ measure are understood to indicate that feature $X_{\\cdot j }$ could  be removed without a significant accuracy loss of our classifier. \n",
    "\n",
    "\n",
    "\n",
    "> **Task 5:** Complete the function ```chiSQ``` to implement the $\\chi^2$ measure for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chiSQ(X, y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X (np.array, n x p): features\n",
    "        y (np.array, n): target\n",
    "        \n",
    "    Returns:\n",
    "        chisq (np.array, p): The ChiSQ mesure for every feature\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    n = X.shape[0]\n",
    "    dim  = X.shape[1]\n",
    "    chisq = np.zeros(dim)\n",
    "    \n",
    "    # Please insert the code for Task 5 here \n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "    return chisq\n",
    "\n",
    "\n",
    "print('The chi squared measure of the first 5 features is\\n',chiSQ(X_train[:,:5],y_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these implementations we are now able to perform feature selection using the $\\chi^2$ measure to select $k$ features to be used for classifcation with $k \\in \\{20,40,60, 80,100,150\\}.$ This allows us to examine the accuracy, AUC and training time values for different values of $k.$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [20, 40, 60, 80, 100, 150]\n",
    "\n",
    "acc = np.zeros(len(k))\n",
    "auc = np.zeros(len(k))\n",
    "time = np.zeros(len(k))\n",
    "\n",
    "fpr = np.zeros((len(k),101))\n",
    "tpr = np.zeros((len(k),101))\n",
    "\n",
    "\n",
    "\n",
    "chisq_values = chiSQ(X,Y)\n",
    "feature_ranking = np.argsort(chisq_values)[::-1]\n",
    "\n",
    "for i, n_features in enumerate(k):\n",
    "    model = LogisticRegression(max_iter=10000)\n",
    "    X_k_train = X_train[:, feature_ranking[:n_features]]\n",
    "    \n",
    "    start = perf_counter()\n",
    "    model.fit(X_k_train, y_train)\n",
    "    end = perf_counter()\n",
    "    \n",
    "    time[i] = end - start\n",
    "    \n",
    "    X_k_test = X_test[:, feature_ranking[:n_features]]\n",
    "    res = model.predict(X_k_test)\n",
    "    \n",
    "    acc[i] = np.sum(res == y_test)/len(y_test)\n",
    "    \n",
    "    \n",
    "    tpr[i], fpr[i], auc[i] = roc_auc(model, X_k_test, y_test.astype('int'))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(17,7))\n",
    "for i in range(6):  \n",
    "    ax[int(i>2), i%3].plot(fpr[i,:], tpr[i,:])\n",
    "    ax[int(i>2), i%3].set_title(\"Top %i features, Acc = %.2f, Auc = %.2f, time= %.2f\"\\\n",
    "                                % (k[i], acc[i], auc[i], time[i]))\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
